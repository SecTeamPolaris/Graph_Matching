{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,shutil\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "        \n",
    "class data_f(object):\n",
    "    def __init__(self):\n",
    "        self.IP = []\n",
    "        self.cert = []\n",
    "        self.urls = []\n",
    "        self.static_features = []\n",
    "        self.seq = []\n",
    "        self.label = []\n",
    "        self.time = []\n",
    "        self.image = []\n",
    "        self.fn = []\n",
    "        \n",
    "    def insert(self,IP,cert,urls,static_features,seq,label,time,image,fn):\n",
    "        self.IP.append(IP)\n",
    "        self.cert.append(cert)\n",
    "        self.urls.append(urls)\n",
    "        self.static_features.append(static_features)\n",
    "        self.seq.append(seq)\n",
    "        self.label.append(label)\n",
    "        self.time.append(time)\n",
    "        self.image.append(image)\n",
    "        self.fn.append(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interval import Interval\n",
    "import os,shutil\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "\n",
    "class cert_cluster(object):\n",
    "    def __init__(self):\n",
    "        self.IPs = []\n",
    "        self.urls = {} #{urls:number}\n",
    "        self.url_number = [] # sorted (url:number)\n",
    "        self.url_weight = {}#{url:weight}\n",
    "        self.url_unique = []\n",
    "        self.certs = [] \n",
    "        self.sessions = []\n",
    "        self.images = []\n",
    "        self.static_features = []\n",
    "        self.fn = []\n",
    "        \n",
    "\n",
    "        self.seq = []\n",
    "        self.label = -1\n",
    "        self.time_slides = [] #(start,end)\n",
    "        \n",
    "        self.sim_list = []\n",
    "        self.url_all = []\n",
    "        \n",
    "    #file name\n",
    "    def fn_cal(self):\n",
    "        self.fn = [ss['fn'] for ss in self.sessions]\n",
    "            \n",
    "        \n",
    "    #all urls\n",
    "    def all_url_cal(self):\n",
    "        for session in self.sessions:\n",
    "            urls = session['urls']\n",
    "            for url in urls:\n",
    "                if url not in self.url_all:\n",
    "                    self.url_all.append(url)\n",
    "    #url sim cal\n",
    "    def sim_cal(self,urls_dict):\n",
    "        self.sim_list = []\n",
    "        for url_dict_per in urls_dict:\n",
    "            score = 0\n",
    "            for url in self.url_unique:\n",
    "                if url in url_dict_per:\n",
    "                    score += 1\n",
    "            self.sim_list.append(score / (len(self.url_unique)*1.0))\n",
    "        if np.array(self.sim_list).max() > 0:\n",
    "            self.sim_list = self.sim_list/np.array(self.sim_list).max()\n",
    "        max_idx = np.argmax(np.array(self.sim_list))\n",
    "        for i in range(len(self.sim_list)):\n",
    "            if i!=max_idx:\n",
    "                self.sim_list[i] = 0\n",
    "            \n",
    "    #ip sim cal\n",
    "    def sim_ip_cal(self,urls_dict):\n",
    "        self.sim_list = []\n",
    "        for url_dict_per in urls_dict:\n",
    "            score = 0\n",
    "            for url in self.IPs:\n",
    "                if url in url_dict_per:\n",
    "                    score += 1\n",
    "            self.sim_list.append(score)\n",
    "        if np.array(self.sim_list).max() > 0:\n",
    "            self.sim_list = self.sim_list/np.array(self.sim_list).max()\n",
    "        max_idx = np.argmax(np.array(self.sim_list))\n",
    "        for i in range(len(self.sim_list)):\n",
    "            if i!=max_idx:\n",
    "                self.sim_list[i] = 0\n",
    "        \n",
    "            \n",
    "    \n",
    "    #static\n",
    "    def static_cal(self):\n",
    "        self.static_features = [a['static_features'] for a in self.sessions]\n",
    "    \n",
    "    \n",
    "    #major voting for cluster's label\n",
    "    def label_cal(self):\n",
    "        labels = [a['label'] for a in self.sessions]\n",
    "        self.label = max(labels,key = labels.count)\n",
    "    \n",
    "    #image extract\n",
    "    def image_cal(self):\n",
    "        self.images = [item['image'].flatten() for item in self.sessions]\n",
    "    \n",
    "    #seq_ipdaate\n",
    "    def seq_cal(self):\n",
    "#         self.seq = [item['seq'].flatten() for item in self.sessions]\n",
    "        for i in range(len(self.sessions)):\n",
    "            session = self.sessions[i]\n",
    "            mat = session['seq']\n",
    "            sta = session['static_features']\n",
    "            for item in sta:\n",
    "                mat = np.append(mat,float(item))\n",
    "\n",
    "            self.seq.append(mat)\n",
    "            \n",
    "            \n",
    "        \n",
    "    #time_slieds generation\n",
    "    def time_cal(self):\n",
    "        def getfirst(item):\n",
    "            return item[0]\n",
    "        for session in self.sessions:\n",
    "            self.time_slides.append(session['time'])\n",
    "        self.time_slides = sorted(self.time_slides,key=getfirst)\n",
    "        \n",
    "        #concat time period\n",
    "        self.time_slides = [Interval(item[0],item[1],lower_closed=True, upper_closed=True) for item in self.time_slides]\n",
    "\n",
    "        while True:\n",
    "            big_flag = 0\n",
    "            for i in range(len(self.time_slides)):\n",
    "                \n",
    "                flag = 0\n",
    "                for j in range(len(self.time_slides)):\n",
    "\n",
    "                    if i ==len(self.time_slides)-1 and j == len(self.time_slides)-1:\n",
    "                        big_flag =1\n",
    "                    if i==j:\n",
    "                        continue      \n",
    "                    if self.time_slides[i].overlaps(self.time_slides[j]):\n",
    "                        interval_merge = self.time_slides[i].join(self.time_slides[j])\n",
    "                        self.time_slides[i] = interval_merge\n",
    "                        del self.time_slides[j]\n",
    "\n",
    "                        flag = 1\n",
    "                        break                \n",
    "                if flag == 1:\n",
    "                    break\n",
    "            if big_flag==1:\n",
    "                break\n",
    "#         print(len(self.time_slides))\n",
    "        \n",
    "                    \n",
    "                    \n",
    "                \n",
    "            \n",
    "            \n",
    "        \n",
    "      \n",
    "    #high-frequency urls calcualte\n",
    "    def urls_cal(self):\n",
    "        for item in self.sessions:\n",
    "            urls = item['urls']\n",
    "            for url in urls:\n",
    "                if url not in list(self.urls.keys()):\n",
    "                    self.urls[url] = 1\n",
    "                else:\n",
    "                    self.urls[url] += 1\n",
    "        #sorted\n",
    "        self.url_number = sorted(self.urls.items(),key=lambda item:item[1],reverse=True)\n",
    "    \n",
    "    #url importance score\n",
    "    def urls_weight(self):\n",
    "        total = np.array([item[1] for item in self.url_number]).sum()*1.0\n",
    "        for item in self.url_number:\n",
    "            self.url_weight[item[0]] = item[1]*1.0/total\n",
    "                \n",
    "#         self.url_weight = [(item[0],item[1]*1.0/total) for item in self.url_number]\n",
    "        self.url_unique = [item[0] for item in self.url_number]\n",
    "        \n",
    "    #IP summarize\n",
    "    def ips_cal(self):\n",
    "        for item in self.sessions:\n",
    "            if item['IP'] not in self.IPs:\n",
    "                self.IPs.append(item['IP'])\n",
    "    \n",
    "    #cert summarize\n",
    "    def cert_cal(self):\n",
    "        for item in self.sessions:\n",
    "            if item['cert'] not in self.certs:\n",
    "                self.certs.append(item['cert'])\n",
    "    \n",
    "    #discard low-frequency urls\n",
    "    def url_clean(self):\n",
    "        if len(self.url_number) <=5:\n",
    "            return\n",
    "        self.url_number = self.url_number[:5]\n",
    "        self.url_unique = [item[0] for item in self.url_number]\n",
    "        self.url_weight = {}\n",
    "        self.urls_weight()\n",
    "        \n",
    "        \n",
    "    \n",
    "    #update cert_clt\n",
    "    def update(self):\n",
    "        self.urls_cal()\n",
    "        self.urls_weight()\n",
    "        self.ips_cal()\n",
    "        self.cert_cal()\n",
    "        self.seq_cal()\n",
    "        self.image_cal()\n",
    "        self.label_cal()\n",
    "        self.static_cal()\n",
    "        self.url_clean()\n",
    "        self.all_url_cal()\n",
    "        self.fn_cal()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering(filename):\n",
    "\n",
    "    data_list = []\n",
    "    dataT = data_f()\n",
    "    with open(filename,'rb') as file:\n",
    "        dataT  = pickle.loads(file.read())\n",
    "\n",
    "    for i in range(len(dataT.label)):\n",
    "        tmp = {}\n",
    "        tmp['IP'] = dataT.IP[i]\n",
    "        tmp['cert'] = dataT.cert[i]\n",
    "        tmp['urls'] = dataT.urls[i]\n",
    "        tmp['static_features'] = dataT.static_features[i]\n",
    "        tmp['seq'] = dataT.seq[i]\n",
    "        tmp['label'] = dataT.label[i]\n",
    "        tmp['time'] = dataT.time[i]\n",
    "        tmp['image'] = dataT.image[i]\n",
    "        tmp['fn'] = dataT.fn[i]\n",
    "\n",
    "        data_list.append(tmp)\n",
    "\n",
    "\n",
    "    #cert clusterting\n",
    "    cert_clusters = []\n",
    "    used_sessions = []\n",
    "    for i in range(len(data_list)):\n",
    "        session = data_list[i]\n",
    "        cert = session['cert']\n",
    "        if cert is 0:\n",
    "            continue\n",
    "        if len(cert_clusters) is 0:\n",
    "            cert_clt = cert_cluster()\n",
    "            cert_clt.sessions.append(session)\n",
    "            cert_clt.cert_cal()\n",
    "            cert_clusters.append(cert_clt)\n",
    "\n",
    "            continue\n",
    "        judge = 0\n",
    "        for j in range(len(cert_clusters)):\n",
    "\n",
    "            if cert in cert_clusters[j].certs:\n",
    "                cert_clusters[j].sessions.append(session)\n",
    "                judge  = 1\n",
    "        if judge is 0:\n",
    "            cert_clt = cert_cluster()\n",
    "            cert_clt.sessions.append(session)\n",
    "            cert_clt.cert_cal()\n",
    "            cert_clusters.append(cert_clt)\n",
    "\n",
    "        used_sessions.append(i)\n",
    "\n",
    "    #update cert_clusters\n",
    "    for j in range(len(cert_clusters)):\n",
    "        cert_clusters[j].update()\n",
    "    #update the remaining sessions in session list\n",
    "    data_list = [data_list[i] for i in range(len(data_list)) if i not in used_sessions]\n",
    "#     print(len(data_list)) \n",
    "\n",
    "\n",
    "    #cert clt clustering\n",
    "    #merge the root cluster pairs with same urls\n",
    "    while len(cert_clusters) >10:\n",
    "        matching_matrix = np.zeros((len(cert_clusters),len(cert_clusters)))\n",
    "        #matching graph construct\n",
    "        for i in range(len(cert_clusters)):\n",
    "            row_clt = cert_clusters[i]\n",
    "            for j in range(len(cert_clusters)):\n",
    "                if i == j :\n",
    "                    continue\n",
    "\n",
    "                column_clt = cert_clusters[j]\n",
    "                r2c = 0\n",
    "                for url_row in row_clt.url_unique:\n",
    "                    if url_row in column_clt.url_unique:\n",
    "                        r2c += column_clt.url_weight[url_row]\n",
    "                matching_matrix[i][j] = r2c\n",
    "\n",
    "        #merge the root cluster pairs with same urls\n",
    "        matching_matrix += matching_matrix.T\n",
    "        matching_matrix = np.triu(matching_matrix).flatten()\n",
    "        max_index = np.argmax(matching_matrix)\n",
    "        i = int(max_index / (len(cert_clusters)))\n",
    "        j = max_index%(len(cert_clusters))\n",
    "\n",
    "        if i==j:\n",
    "            break\n",
    "        #merge\n",
    "        cert_clusters[i].sessions.extend(cert_clusters[j].sessions)\n",
    "        #æ›´update root clusters clt\n",
    "        cert_clusters[i].update()\n",
    "        del cert_clusters[j]\n",
    "\n",
    "    #gather ip to cert\n",
    "    used_sessions = []\n",
    "    for i in range(len(data_list)):\n",
    "        session = data_list[i]\n",
    "        ip = session['IP']\n",
    "\n",
    "        for j in range(len(cert_clusters)):\n",
    "\n",
    "            if ip in cert_clusters[j].IPs:\n",
    "                cert_clusters[j].sessions.append(session)\n",
    "                used_sessions.append(i)\n",
    "\n",
    "                #update features in cert_clusters\n",
    "                cert_clusters[j].update()\n",
    "                break\n",
    "\n",
    "    #update the remaining sessions in session list\n",
    "    data_list = [data_list[i] for i in range(len(data_list)) if i not in used_sessions]\n",
    "#     print(len(data_list))\n",
    "\n",
    "\n",
    "    #urls\n",
    "    for i in range(len(cert_clusters)):\n",
    "        cert_clusters[i].url_clean()\n",
    "\n",
    "    #concrete matching\n",
    "\n",
    "    from fuzzywuzzy import fuzz\n",
    "    \n",
    "    used_sessions = []\n",
    "    for i in range(len(data_list)):\n",
    "        session = data_list[i]\n",
    "        urls = session['urls']\n",
    "        if len(urls) is 0:\n",
    "            continue\n",
    "\n",
    "        cert_clt_can = -1\n",
    "        can_max = 0\n",
    "        for j in range(len(cert_clusters)):\n",
    "            cert_clt = cert_clusters[j]\n",
    "\n",
    "            match_per = 0\n",
    "            for url in urls:\n",
    "                if url in cert_clt.url_unique:\n",
    "                    match_per+=1\n",
    "            if match_per > can_max:\n",
    "                can_max = match_per\n",
    "                cert_clt_can = j\n",
    "\n",
    "        if cert_clt_can is -1:\n",
    "            continue\n",
    "\n",
    "        cert_clusters[cert_clt_can].sessions.append(data_list[i])\n",
    "\n",
    "        #update cert_clusters\n",
    "        cert_clt.update()\n",
    "\n",
    "        used_sessions.append(i)\n",
    "\n",
    "    data_list = [data_list[i] for i in range(len(data_list)) if i not in used_sessions]\n",
    "#     print(len(data_list))\n",
    "\n",
    "    for i in range(len(cert_clusters)):\n",
    "        cert_clusters[i].url_clean()\n",
    "\n",
    "\n",
    "\n",
    "    while len(cert_clusters) >10:\n",
    "        matching_matrix = np.zeros((len(cert_clusters),len(cert_clusters)))\n",
    "\n",
    "        for i in range(len(cert_clusters)):\n",
    "            row_clt = cert_clusters[i]\n",
    "            for j in range(len(cert_clusters)):\n",
    "                if i == j :\n",
    "                    continue\n",
    "\n",
    "                column_clt = cert_clusters[j]\n",
    "                r2c = 0\n",
    "                for url_row in row_clt.url_unique:\n",
    "                    if url_row in column_clt.url_unique:\n",
    "                        r2c += column_clt.url_weight[url_row]\n",
    "                matching_matrix[i][j] = r2c\n",
    "\n",
    "\n",
    "        matching_matrix += matching_matrix.T\n",
    "        matching_matrix = np.triu(matching_matrix).flatten()\n",
    "        max_index = np.argmax(matching_matrix)\n",
    "        i = int(max_index / (len(cert_clusters)))\n",
    "        j = max_index%(len(cert_clusters))\n",
    "\n",
    "        if i==j:\n",
    "            break\n",
    "\n",
    "        cert_clusters[i].sessions.extend(cert_clusters[j].sessions)\n",
    "\n",
    "        cert_clusters[i].update()\n",
    "\n",
    "        del cert_clusters[j]\n",
    "\n",
    "\n",
    "    url_clusters = []\n",
    "    used_sessions = []\n",
    "\n",
    "    #inital url clusters\n",
    "    for i in range(len(data_list)):\n",
    "        session = data_list[i]\n",
    "        if len(session['urls']) == 0:\n",
    "            continue\n",
    "        used_sessions.append(i)\n",
    "        clt = cert_cluster()\n",
    "        clt.sessions.append(session)\n",
    "        clt.update()\n",
    "        url_clusters.append(clt)\n",
    "\n",
    "    while True:\n",
    "        matching_matrix = np.zeros((len(url_clusters),len(url_clusters)))\n",
    "\n",
    "        for i in range(len(url_clusters)):\n",
    "            row_clt = url_clusters[i]\n",
    "            for j in range(len(url_clusters)):\n",
    "                if i == j :\n",
    "                    continue\n",
    "\n",
    "                column_clt = url_clusters[j]\n",
    "                r2c = 0\n",
    "                for url_row in row_clt.url_unique:\n",
    "                    if url_row in column_clt.url_unique:\n",
    "                        r2c += column_clt.url_weight[url_row]\n",
    "                matching_matrix[i][j] = r2c\n",
    "\n",
    "\n",
    "        matching_matrix += matching_matrix.T\n",
    "        matching_matrix = np.triu(matching_matrix).flatten()\n",
    "        max_index = np.argmax(matching_matrix)\n",
    "        i = int(max_index / (len(url_clusters)))\n",
    "        j = max_index%(len(url_clusters))\n",
    "\n",
    "        if i==j:\n",
    "            break\n",
    "\n",
    "        url_clusters[i].sessions.extend(url_clusters[j].sessions)\n",
    "\n",
    "        url_clusters[i].update()\n",
    "\n",
    "        del url_clusters[j]\n",
    "\n",
    "    data_list = [data_list[i] for i in range(len(data_list)) if i not in used_sessions]\n",
    "\n",
    "\n",
    "\n",
    "    clusters = []\n",
    "    clusters.extend(cert_clusters)\n",
    "    clusters.extend(url_clusters)\n",
    "\n",
    "    used_sessions = []\n",
    "    for i in range(len(data_list)):\n",
    "        session = data_list[i]\n",
    "        ip = session['IP']\n",
    "\n",
    "        for j in range(len(clusters)):\n",
    "    #         cert_clt = clusters[j]\n",
    "            if ip in clusters[j].IPs:\n",
    "                clusters[j].sessions.append(session)\n",
    "                used_sessions.append(i)\n",
    "\n",
    "                #update cert_clusters\n",
    "                clusters[j].update()\n",
    "                break\n",
    "\n",
    "    #update the remaining sessions in session list\n",
    "    data_list = [data_list[i] for i in range(len(data_list)) if i not in used_sessions]\n",
    "#     print(len(data_list))\n",
    "\n",
    "    IP_clusters = []\n",
    "    used_sessions = []\n",
    "\n",
    "    #inital IP clusters\n",
    "    for i in range(len(data_list)):\n",
    "        session = data_list[i]\n",
    "        IP = session['IP']\n",
    "        if len(IP_clusters) == 0:\n",
    "            used_sessions.append(i)\n",
    "            clt = cert_cluster()\n",
    "            clt.sessions.append(session)\n",
    "            clt.update()\n",
    "            IP_clusters.append(clt)\n",
    "        else:\n",
    "            d = 0\n",
    "            for j in range(len(IP_clusters)):\n",
    "                if IP in IP_clusters[j].IPs:\n",
    "                    IP_clusters[j].sessions.append(session)\n",
    "                    IP_clusters[j].update()\n",
    "                    used_sessions.append(i)\n",
    "                    d = 1\n",
    "                    break\n",
    "\n",
    "            if d == 0:\n",
    "                used_sessions.append(i)\n",
    "                clt = cert_cluster()\n",
    "                clt.sessions.append(session)\n",
    "                clt.update()\n",
    "                IP_clusters.append(clt)\n",
    "\n",
    "    data_list = [data_list[i] for i in range(len(data_list)) if i not in used_sessions]\n",
    "#     print(len(data_list))\n",
    "\n",
    "    clusters.extend(IP_clusters)\n",
    "\n",
    "    sorted_clusters = sorted(clusters,key=lambda item:len(item.sessions),reverse=False)\n",
    "\n",
    "    while len(sorted_clusters) > 10:\n",
    "        judge = 0\n",
    "        sorted_clusters = sorted(sorted_clusters,key=lambda item:len(item.sessions),reverse=False)\n",
    "        for i in range(len(sorted_clusters)):\n",
    "            judge = 0\n",
    "            match_num = 0\n",
    "            match_idx = -1\n",
    "            for j in range(len(sorted_clusters)):\n",
    "                if i==j:\n",
    "                    continue\n",
    "                match = len(set(sorted_clusters[i].url_unique)&set(sorted_clusters[j].url_unique))\n",
    "                if match>match_num:\n",
    "                    match_idx = j\n",
    "                    match_num = match\n",
    "                    judge =1\n",
    "\n",
    "\n",
    "            if judge == 1:\n",
    "                sorted_clusters[match_idx].sessions.extend(sorted_clusters[i].sessions)\n",
    "                sorted_clusters[match_idx].update()\n",
    "\n",
    "                del sorted_clusters[i]\n",
    "                break\n",
    "        if judge ==0:\n",
    "            break\n",
    "\n",
    "\n",
    "#     print(len(sorted_clusters))           \n",
    "\n",
    "\n",
    "    dataT = data_f()\n",
    "\n",
    "    with open(\"data/InitTraffic.pkl\",'rb') as file:\n",
    "        dataT  = pickle.loads(file.read())\n",
    "\n",
    "\n",
    "    #ip dict   \n",
    "    data_list = []\n",
    "\n",
    "    for i in range(len(dataT.label)):\n",
    "        tmp = {}\n",
    "        tmp['IP'] = dataT.IP[i]\n",
    "        tmp['cert'] = dataT.cert[i]\n",
    "        tmp['urls'] = dataT.urls[i]\n",
    "        tmp['static_features'] = dataT.static_features[i]\n",
    "        tmp['seq'] = dataT.seq[i]\n",
    "        tmp['label'] = dataT.label[i]\n",
    "        tmp['time'] = dataT.time[i]\n",
    "        tmp['image'] = dataT.image[i]\n",
    "        tmp['fn'] = dataT.fn[i]\n",
    "\n",
    "        data_list.append(tmp)\n",
    "#     print(len(data_list))\n",
    "\n",
    "# UMT construction\n",
    "    clusters_dict = [cert_cluster() for i in range(10)]\n",
    "    for item in data_list:\n",
    "        idx = item['label']\n",
    "        clusters_dict[idx].sessions.append(item)\n",
    "\n",
    "    for i in range(len(clusters_dict)):\n",
    "        clusters_dict[i].update()\n",
    "\n",
    "    urls_dict = []                             \n",
    "    for clt in clusters_dict:\n",
    "        urls_dict.append(clt.IPs)\n",
    "\n",
    "    for i in range(len(sorted_clusters)):\n",
    "    #     with_urls_list[i].sim_cal(urls_dict)\n",
    "        sorted_clusters[i].sim_ip_cal(urls_dict)\n",
    "\n",
    "    #construct sim matrix\n",
    "    while len(sorted_clusters)>10:\n",
    "        matching_matrix = np.zeros((len(sorted_clusters),len(sorted_clusters)))\n",
    "\n",
    "        for i in range(len(sorted_clusters)):\n",
    "            row_clt = sorted_clusters[i]\n",
    "            for j in range(len(sorted_clusters)):\n",
    "                if i == j :\n",
    "                    continue\n",
    "\n",
    "                column_clt = sorted_clusters[j]\n",
    "\n",
    "                if np.array(row_clt.sim_list).sum() == 0 or np.array(column_clt.sim_list).sum() == 0:\n",
    "                    sim = 0.0\n",
    "                else:\n",
    "                    sim = cosine_similarity(np.array(row_clt.sim_list).reshape(1,-1),np.array(column_clt.sim_list).reshape(1,-1))\n",
    "\n",
    "                matching_matrix[i][j] = sim\n",
    "\n",
    "\n",
    "        matching_matrix += matching_matrix.T\n",
    "        matching_matrix = np.triu(matching_matrix).flatten()\n",
    "        max_index = np.argmax(matching_matrix)\n",
    "        i = int(max_index / (len(sorted_clusters)))\n",
    "        j = max_index%(len(sorted_clusters))\n",
    "\n",
    "        if i==j:\n",
    "            break\n",
    "\n",
    "        sorted_clusters[i].sessions.extend(sorted_clusters[j].sessions)\n",
    "\n",
    "        sorted_clusters[i].update()\n",
    "        sorted_clusters[i].sim_ip_cal(urls_dict)\n",
    "\n",
    "        del sorted_clusters[j]\n",
    "\n",
    "    clusters_dict = [cert_cluster() for i in range(10)]\n",
    "    for item in data_list:\n",
    "        idx = item['label']\n",
    "        clusters_dict[idx].sessions.append(item)\n",
    "\n",
    "    for i in range(len(clusters_dict)):\n",
    "        clusters_dict[i].update()\n",
    "\n",
    "    urls_dict = []\n",
    "\n",
    "    for i in range(len(clusters_dict)):\n",
    "        clt = clusters_dict[i]\n",
    "        urls_dict.append([item for item in clt.url_all])\n",
    "\n",
    "\n",
    "    # sorted_cluster matching \n",
    "    # seperate sorted_cluster into 2 lists(with url and without url)\n",
    "    with_urls_list = []\n",
    "    wo_urls_list = []\n",
    "    for clt in sorted_clusters:\n",
    "        if len(clt.url_unique) == 0:\n",
    "            wo_urls_list.append(clt)\n",
    "        else:\n",
    "            with_urls_list.append(clt)\n",
    "\n",
    "\n",
    "    # we aim to cluster clts into 6 clts in  with_urls_list by urls_dict \n",
    "    for i in range(len(with_urls_list)):\n",
    "        with_urls_list[i].sim_cal(urls_dict)\n",
    "\n",
    "\n",
    "    #construct sim matrix\n",
    "    while len(with_urls_list)>10:\n",
    "        matching_matrix = np.zeros((len(with_urls_list),len(with_urls_list)))\n",
    "\n",
    "        for i in range(len(with_urls_list)):\n",
    "            row_clt = with_urls_list[i]\n",
    "            for j in range(len(with_urls_list)):\n",
    "                if i == j :\n",
    "                    continue\n",
    "\n",
    "                column_clt = with_urls_list[j]\n",
    "                if np.array(row_clt.sim_list).sum() == 0 or np.array(column_clt.sim_list).sum() == 0:\n",
    "                    sim = 0.0\n",
    "                else:\n",
    "                    sim = cosine_similarity(np.array(row_clt.sim_list).reshape(1,-1),np.array(column_clt.sim_list).reshape(1,-1))\n",
    "\n",
    "                matching_matrix[i][j] = sim\n",
    "\n",
    "\n",
    "        matching_matrix += matching_matrix.T\n",
    "        matching_matrix = np.triu(matching_matrix).flatten()\n",
    "        max_index = np.argmax(matching_matrix)\n",
    "        i = int(max_index / (len(with_urls_list)))\n",
    "        j = max_index%(len(with_urls_list))\n",
    "\n",
    "        if i==j:\n",
    "            break\n",
    "\n",
    "        with_urls_list[i].sessions.extend(with_urls_list[j].sessions)\n",
    "\n",
    "        with_urls_list[i].update()\n",
    "        with_urls_list[i].sim_cal(urls_dict)\n",
    "\n",
    "        del with_urls_list[j]\n",
    "\n",
    "    return with_urls_list,wo_urls_list,urls_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "def matching_process(G_a, G_b,c):\n",
    "    '''\n",
    "    Input: Two candidate graphs for matching with same shape. (N*N)\n",
    "    Output: matching list of vertices from Graph A and Graph B.\n",
    "    '''\n",
    "    \n",
    "    # matrix similarity\n",
    "\n",
    "    def M_measure(As_a,As_b):\n",
    "        '''\n",
    "        Input: Assigment pair with shape of As_a =  (G_a_vertice, G_b_vertice)\n",
    "        Output: M(As_a,As_b)\n",
    "        '''\n",
    "    \n",
    "        A_src_index = As_a[0]\n",
    "        A_dst_index = As_a[1]\n",
    "\n",
    "        B_src_index = As_b[0]\n",
    "        B_dst_index = As_b[1]\n",
    "        \n",
    "#         return math.exp(-(G_a[A_src_index][A_dst_index] - G_b[B_src_index][B_dst_index])**2 / (2*c*c))\n",
    "\n",
    "        return math.exp(-(G_a[A_src_index][B_src_index] - G_b[A_dst_index][B_dst_index])**2 / (2*c*c))\n",
    "    \n",
    "    #construct Assigments list by travelling all vertice pairs. Length of N*N.\n",
    "    As_list = []\n",
    "    for i in range(G_a.shape[0]):\n",
    "        for j in range(G_a.shape[1]):\n",
    "            As_list.append((i,j))\n",
    "#     print(As_list)\n",
    "    \n",
    "    #construct Assigment similarity by travelling all assigments in assigments list, with shape of (N2,N2).\n",
    "    M = []\n",
    "    for item_left in As_list:\n",
    "        for item_right in As_list:\n",
    "            M.append(M_measure(item_left,item_right))\n",
    "#             print(item_left,item_right)\n",
    "    \n",
    "    M = np.array(M)\n",
    "    M = M.reshape((G_a.shape[0]**2, G_a.shape[0]**2))\n",
    "#     print(As_list)\n",
    "#     print(M)\n",
    "    \n",
    "    # caculate principle eigenvector of M\n",
    "#     print(M == M.T)\n",
    "    eigenvalue,featurevector = np.linalg.eig(M)\n",
    "    eigen_max_index = np.argmax(eigenvalue)\n",
    "#     print(eigenvalue)\n",
    "#     print(featurevector)\n",
    "#     print(eigen_max_index)\n",
    "    \n",
    "    \n",
    "    \n",
    "    prl_eigenvector = featurevector.T[eigen_max_index]\n",
    "    \n",
    "    prl_eigenvector = prl_eigenvector.tolist()\n",
    "#     print(prl_eigenvector)\n",
    "    #sort principle eigenvector M by value\n",
    "    sort_index_M = [(item,prl_eigenvector.index(item)) for item in prl_eigenvector]\n",
    "#     print(sort_index_M)\n",
    "    \n",
    "    def get_first(item):\n",
    "        return item[0]\n",
    "    #sort_index_M is the sorted assigments index \n",
    "    sort_index_M = sorted(sort_index_M, key=get_first, reverse=True)\n",
    "    sort_index_M = [item[1] for item in sort_index_M]\n",
    "    \n",
    "    #accepting and rejecting process\n",
    "    As_list_accept_status = [0]*len(As_list)\n",
    "    G_a_matched_index = []\n",
    "    G_b_matched_index = []\n",
    "    \n",
    "    matching_list = [-1] * G_a.shape[0]\n",
    "    for index in sort_index_M:\n",
    "        match = As_list[index]\n",
    "#         if  match[1] in G_b_matched_index:\n",
    "#             continue\n",
    "        if match[0] in G_a_matched_index or match[1] in G_b_matched_index:\n",
    "            continue\n",
    "        matching_list[match[1]] = match[0]\n",
    "        \n",
    "        G_a_matched_index.append(match[0])\n",
    "        G_b_matched_index.append(match[1])\n",
    "    \n",
    "    \n",
    "    return matching_list\n",
    "        \n",
    "def count_acc(y_real,y_pred):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "    for i in range(y_real.shape[0]):\n",
    "        if y_real[i] == y_pred[i]:\n",
    "            tp += 1\n",
    "    \n",
    "    \n",
    "#     Precision = tp/(tp+fp)\n",
    "#     Recall = tp/(tp+fn)\n",
    "#     f1_score = 2*Precision*Recall/(Precision+Recall)\n",
    "    Acc = (tp)/(y_real.shape[0])\n",
    "    print(\"Overall Accuracy: {} \".format(Acc))\n",
    "#     print(tp)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "import random\n",
    "from simhash import Simhash\n",
    "\n",
    "#dimension [urls, IP, static, seq, image]\n",
    "def clt_sim(clt1,clt2,url_dict=None):\n",
    "    sim_vec = [0]*1\n",
    "    \n",
    "    #urls simhash\n",
    "#     if len(clt1.urls) == 0 or len(clt2.urls) == 0:\n",
    "#         sim_vec[0] = 0\n",
    "#     else:\n",
    "#         num = 2\n",
    "#         if len(clt1.url_unique)<num:\n",
    "#             num = len(clt1.url_unique)\n",
    "#         if len(clt2.url_unique)<num:\n",
    "#             num = len(clt1.url_unique)\n",
    "#         urls1 = clt1.url_unique[:2]\n",
    "#         urls2 = clt2.url_unique[:2]\n",
    "#         print(urls1)\n",
    "#         print(urls2)\n",
    "        \n",
    "        \n",
    "#         total = 0.0\n",
    "#         for url1 in urls1:\n",
    "#             for url2 in urls2:\n",
    "# #                 total.append((fuzz.ratio(url1,url2)+fuzz.ratio(url2,url1))/200.0)\n",
    "# #                 total.append(1.0 - Simhash(url1).distance(Simhash(url2))/100.0)\n",
    "# #                 total += (fuzz.ratio(url1,url2)+fuzz.ratio(url2,url1)) /200.0\n",
    "#                 total += 1.0 - Simhash(url1).distance(Simhash(url2))/100.0\n",
    "        \n",
    "#         total = total / (len(urls1) * len(urls2) *1.0)\n",
    "# #         total = np.array(total).max()\n",
    "#         print(total)\n",
    "#         print()\n",
    "#         sim_vec[0] = total\n",
    "        \n",
    "    #compare with dict\n",
    "    if len(clt1.urls) == 0 or len(clt2.urls) == 0:\n",
    "        sim_vec[0] = 0\n",
    "    else:\n",
    "        num = 2\n",
    "        if len(clt1.url_unique)<num:\n",
    "            num = len(clt1.url_unique)\n",
    "        if len(clt2.url_unique)<num:\n",
    "            num = len(clt1.url_unique)\n",
    "        urls1 = clt1.url_unique[:2]\n",
    "        urls2 = clt2.url_unique[:2]\n",
    "    \n",
    "    vec1 = np.zeros(10)\n",
    "    for i in range(10):\n",
    "        m = []\n",
    "        for url in urls1:\n",
    "            m1 = []\n",
    "            for dict_url in url_dict[i]:\n",
    "                sim = 1.0 - Simhash(url).distance(Simhash(dict_url))/100.0\n",
    "                m1.append(sim)\n",
    "#                 if sim >m1:\n",
    "#                     m1 = sim\n",
    "            m1 = np.array(m1).mean()\n",
    "            m.append(m1)\n",
    "#             if m1>m:\n",
    "#                 m = m1\n",
    "        vec1[i] = np.array(m).mean()\n",
    "#         vec1[i] = m\n",
    "    \n",
    "    vec2 = np.zeros(10)\n",
    "    for i in range(10):\n",
    "        m = []\n",
    "        for url in urls2:\n",
    "            m1 = []\n",
    "            for dict_url in url_dict[i]:\n",
    "                sim = 1.0 - Simhash(url).distance(Simhash(dict_url))/100.0\n",
    "                m1.append(sim)\n",
    "#                 if sim >m1:\n",
    "#                     m1 = sim\n",
    "            m1 = np.array(m1).mean()\n",
    "            m.append(m1)\n",
    "#             if m1>m:\n",
    "#                 m = m1\n",
    "        vec2[i] = np.array(m).mean()\n",
    "#     print(vec1)\n",
    "#     print(vec2)\n",
    "#     print()\n",
    "    sim_vec[0] = cosine_similarity(vec1.reshape(1, -1),vec2.reshape(1, -1))\n",
    "#     print(sim_vec[0])\n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "#     #IP sim\n",
    "#     IPs1 = clt1.IPs\n",
    "#     IPs2 = clt2.IPs\n",
    "        \n",
    "#     total = 0.0\n",
    "#     for IP1 in IPs1:\n",
    "#         for IP2 in IPs2:\n",
    "# #             total += (fuzz.ratio(IP1,IP2)+fuzz.ratio(IP2,IP1))/200.0\n",
    "#             total += 1.0 - Simhash(IP1).distance(Simhash(IP2))/100.0\n",
    "        \n",
    "#     total = total / (len(IPs1) * len(IPs2) *1.0)\n",
    "#     sim_vec[1] = total\n",
    "    \n",
    "    #static vector consine\n",
    "#     total = 0.0\n",
    "#     for f1 in clt1.static_features:\n",
    "#         for f2 in clt2.static_features:\n",
    "#             total += cosine_similarity(f1.reshape(1,-1),f2.reshape(1,-1))\n",
    "#     total = total / (len(clt1.static_features) * len(clt2.static_features) *1.0)\n",
    "#     sim_vec[2] = total[0][0]\n",
    "    \n",
    "# #     sim_vec[2] = cosine_similarity(np.array(clt1.static_features).mean(axis=0).reshape(1,-1),np.array(clt2.static_features).mean(axis=0).reshape(1,-1))\n",
    "    \n",
    "# #     seq consine\n",
    "#     sim_vec[3] = cosine_similarity(np.array(clt1.seq).mean(axis=0).reshape(1,-1),np.array(clt2.seq).mean(axis=0).reshape(1,-1))[0][0]\n",
    "#     total = 0.0\n",
    "#     print((len(clt1.seq)))\n",
    "#     print((len(clt2.seq)))\n",
    "#     for f1 in clt1.seq:\n",
    "#         for f2 in clt2.seq:\n",
    "#             total += cosine_similarity(f1.reshape(1,-1),f2.reshape(1,-1))\n",
    "#     total = total / (len(clt1.seq) * len(clt2.seq) *1.0)\n",
    "#     sim_vec[3] = total\n",
    "    \n",
    "    #image consine\n",
    "#     sim_vec[2] = cosine_similarity(np.array(clt1.images).mean(axis=0).reshape(1,-1),np.array(clt2.images).mean(axis=0).reshape(1,-1))[0][0]\n",
    "#     total = 0.0\n",
    "#     for f1 in clt1.images:\n",
    "#         for f2 in clt2.images:\n",
    "#             total += cosine_similarity(f1.reshape(1,-1),f2.reshape(1,-1))\n",
    "#     total = total / (len(clt1.images) * len(clt2.images) *1.0)\n",
    "#     sim_vec[0] = total[0][0]\n",
    "    return  np.array(sim_vec).mean()\n",
    "#     return sim_vec[0]\n",
    "\n",
    "def clt_sim_same(clt1):\n",
    "    sim_vec = [0]*1\n",
    "    \n",
    "    #urls\n",
    "    if len(clt1.urls) == 0:\n",
    "        sim_vec[0] = 0\n",
    "    else:\n",
    "        urls1 = clt1.url_unique[:2]\n",
    "        \n",
    "        total = 0.0\n",
    "        for url1 in urls1:\n",
    "            for url2 in urls1:\n",
    "#                 total.append((fuzz.ratio(url1,url2)+fuzz.ratio(url2,url1))/200.0)\n",
    "#                 total.append(1.0 - Simhash(url1).distance(Simhash(url2))/100.0)\n",
    "#                 total += (fuzz.ratio(url1,url2)+fuzz.ratio(url2,url1))/200.0\n",
    "                total += 1.0 - Simhash(url1).distance(Simhash(url2))/100.0\n",
    "        \n",
    "        total = total / (len(urls1) * len(urls1) *1.0)\n",
    "#         total = np.array(total).max()\n",
    "        sim_vec[0] = total\n",
    "#         print(urls1)\n",
    "#         print(total)\n",
    "#         print()\n",
    "    \n",
    "    \n",
    "        \n",
    "    #IPs\n",
    "#     IPs1 = clt1.IPs\n",
    "        \n",
    "#     total = 0.0\n",
    "#     for IP1 in IPs1:\n",
    "#         for IP2 in IPs1:\n",
    "# #             total += (fuzz.ratio(IP1,IP2)+fuzz.ratio(IP2,IP1))/200.0\n",
    "#             total += 1.0 - Simhash(IP1).distance(Simhash(IP2))/100.0\n",
    "        \n",
    "#     total = total / (len(IPs1) * len(IPs1) *1.0)\n",
    "#     sim_vec[1] = total\n",
    "    \n",
    "#     #static vector consine\n",
    "#     proto = np.array(clt1.static_features).mean(axis=0).reshape(1,-1)\n",
    "#     total = 0.0\n",
    "    \n",
    "#     for static_features in clt1.static_features:\n",
    "#         total += cosine_similarity(proto,np.array(static_features).reshape(1,-1))\n",
    "#     sim_vec[2] =  total[0][0] / (len(clt1.static_features) *1.0)\n",
    "    \n",
    "#     #static vector consine\n",
    "#     proto = np.array(clt1.seq).mean(axis=0).reshape(1,-1)\n",
    "#     total = 0.0\n",
    "    \n",
    "#     for seq in clt1.seq:\n",
    "#         total += cosine_similarity(proto,np.array(seq).reshape(1,-1))\n",
    "#     sim_vec[3] =  total[0][0] / (len(clt1.seq) *1.0)\n",
    "    \n",
    "    \n",
    "    #images\n",
    "#     proto = np.array(clt1.images).mean(axis=0).reshape(1,-1)\n",
    "#     total = 0.0\n",
    "    \n",
    "#     for image in clt1.images:\n",
    "#         total += cosine_similarity(proto,np.array(image).reshape(1,-1))\n",
    "#     sim_vec[0] =  total[0][0] / (len(clt1.images) *1.0)\n",
    "    \n",
    "    return np.array(sim_vec).mean()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepro(with_urls_list_normal,with_urls_list_poor):\n",
    "    with_urls_list_normal_sorted = []\n",
    "\n",
    "    for i in range(len(with_urls_list_normal)):\n",
    "        for item in with_urls_list_normal:\n",
    "            if item.label == i:\n",
    "                with_urls_list_normal_sorted.append(item)\n",
    "\n",
    "    with_urls_list_poor_sorted = []\n",
    "\n",
    "    for i in range(len(with_urls_list_poor)):\n",
    "        for item in with_urls_list_poor:\n",
    "            if item.label == i:\n",
    "                with_urls_list_poor_sorted.append(item)\n",
    "    return with_urls_list_normal_sorted, with_urls_list_poor_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_trans(with_urls_list_normal,with_urls_list_poor,urls_dict):\n",
    "    matching_matrix_normal = np.zeros((len(with_urls_list_normal),len(with_urls_list_normal)))\n",
    "    #construct SimGraph\n",
    "    for i in range(len(with_urls_list_normal)):\n",
    "        row_clt = with_urls_list_normal[i]\n",
    "        for j in range(len(with_urls_list_normal)):\n",
    "            column_clt = with_urls_list_normal[j]\n",
    "\n",
    "            if i==j:\n",
    "                sim = clt_sim_same(with_urls_list_normal[i])\n",
    "            else:\n",
    "                sim = clt_sim(with_urls_list_normal[i],with_urls_list_normal[j],urls_dict)\n",
    "\n",
    "    #         sim = cosine_similarity(np.array(with_urls_list_normal[i].images).mean(axis=0).reshape(1,-1),np.array(with_urls_list_normal[j].images).mean(axis=0).reshape(1,-1))\n",
    "            matching_matrix_normal[i][j] = sim\n",
    "    #         print(sim)\n",
    "\n",
    "\n",
    "    matching_matrix_poor = np.zeros((len(with_urls_list_poor),len(with_urls_list_poor)))\n",
    "    #construct SimGraph\n",
    "    for i in range(len(with_urls_list_poor)):\n",
    "        row_clt = with_urls_list_poor[i]\n",
    "        for j in range(len(with_urls_list_poor)):\n",
    "            column_clt = with_urls_list_poor[j]\n",
    "            if i==j:\n",
    "                sim = clt_sim_same(with_urls_list_poor[i])\n",
    "            else:\n",
    "                sim = clt_sim(with_urls_list_poor[i],with_urls_list_poor[j],urls_dict)\n",
    "    #         sim = cosine_similarity(np.array(with_urls_list_poor[i].images).mean(axis=0).reshape(1,-1),np.array(with_urls_list_poor[j].images).mean(axis=0).reshape(1,-1))\n",
    "            matching_matrix_poor[i][j] = sim\n",
    "\n",
    "    return matching_matrix_normal,matching_matrix_poor, matching_process(matching_matrix_normal,matching_matrix_poor, cosine_similarity(matching_matrix_normal.flatten().reshape(1, -1),matching_matrix_poor.flatten().reshape(1, -1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clt_final(with_urls_list_poor,wo_urls_list_poor):\n",
    "    def image_sim(clt1,clt2):\n",
    "        total = 0.0\n",
    "        for f1 in clt1.images:\n",
    "            for f2 in clt2.images:\n",
    "                total += cosine_similarity(f1.reshape(1,-1),f2.reshape(1,-1))\n",
    "        total = total / (len(clt1.images) * len(clt2.images) *1.0)\n",
    "        return total\n",
    "\n",
    "    def time_sim(clt1,clt2):\n",
    "        total = 0\n",
    "        for time_row in clt1.time_slides:\n",
    "            for time_col in clt2.time_slides:\n",
    "                if time_row.overlaps(time_col):\n",
    "                    r2c += 1\n",
    "        return total\n",
    "\n",
    "\n",
    "        for i in range(len(clt1)):\n",
    "            row_clt = clt[i]\n",
    "            for j in range(len(sorted_clusters)):\n",
    "                if i == j :\n",
    "                    continue\n",
    "\n",
    "                column_clt = sorted_clusters[j]\n",
    "                r2c = 0\n",
    "                for time_row in row_clt.time_slides:\n",
    "                    for time_col in column_clt.time_slides:\n",
    "                        if time_row.overlaps(time_col):\n",
    "                            r2c += 1\n",
    "\n",
    "   #split no url clusters\n",
    "    data_list_no_url = []\n",
    "    for clt in wo_urls_list_poor:\n",
    "        data_list_no_url.extend(clt.sessions)\n",
    "\n",
    "    #one session in each cluster\n",
    "    new_clusters_no_url = []\n",
    "    for data in data_list_no_url:\n",
    "        tmp = cert_cluster()\n",
    "        tmp.sessions.append(data)\n",
    "        tmp.update()\n",
    "        new_clusters_no_url.append(tmp)\n",
    "\n",
    "\n",
    "    #Similarity compution\n",
    "    corr = 0\n",
    "\n",
    "\n",
    "    for clt in new_clusters_no_url:\n",
    "        sim_list = [time_sim(clt,item) for item in with_urls_list_poor]\n",
    "#         sim_list = [cosine_similarity(np.array(clt.images).mean(axis=0).reshape(1,-1),np.array(item.images).mean(axis=0).reshape(1,-1))[0][0]\n",
    "#                    for item in with_urls_list_poor]\n",
    "        matched_idx = np.argmax(np.array(sim_list))\n",
    "        if clt.label == with_urls_list_poor[matched_idx].label:\n",
    "            corr += 1\n",
    "    \n",
    "        #gather to the roor clusters\n",
    "        with_urls_list_poor[matched_idx].sessions.extend(clt.sessions)\n",
    "    return with_urls_list_poor\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix as conm\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score\n",
    "\n",
    "def indicator_cal(y_true,y_pred):\n",
    "    \n",
    "#     y_pred = pred.flatten().tolist()\n",
    "#     y_true = label.flatten().tolist()\n",
    "    acc = accuracy_score(y_true,y_pred)\n",
    "    pre = precision_score(y_true,y_pred,average='weighted')\n",
    "    rec = recall_score(y_true,y_pred,average='weighted')\n",
    "    f1 = (2.0*pre*rec/(pre+rec))\n",
    "    \n",
    "    return acc,pre,rec,f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_cal(with_urls_list_normal,with_urls_list_poor,matched_list):\n",
    "    ground_truth = []\n",
    "    predict = []\n",
    "\n",
    "    for i in range(len(matched_list)): \n",
    "        tru_label = [item['label'] for item in with_urls_list_poor[matched_list[i]].sessions]\n",
    "        prd_label = [with_urls_list_normal[i].label]*len(tru_label)\n",
    "        predict.extend(prd_label)\n",
    "        ground_truth.extend(tru_label)\n",
    "\n",
    "    ground_truth  = np.array(ground_truth)\n",
    "    predict = np.array(predict)\n",
    "\n",
    "    return indicator_cal(ground_truth,predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph matching in same network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Processing...\n",
      "Done!\n",
      "Matching...\n",
      "Done!\n",
      "Clustering isolate sessions...\n",
      "Done!\n",
      "Calculating results...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9973591549295775,\n",
       " 0.9974311779769526,\n",
       " 0.9973591549295775,\n",
       " 0.9973951651530485)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Cluster Processing...\")\n",
    "with_urls_list_normal, wo_urls_list_normal, urls_dict = clustering(\"data/InitTraffic_train.pkl\")\n",
    "with_urls_list_poor, wo_urls_list_poor, urls_dict = clustering(\"data/InitTraffic_test.pkl\")\n",
    "with_urls_list_normal, with_urls_list_poor = prepro(with_urls_list_normal,with_urls_list_poor)\n",
    "print(\"Done!\")\n",
    "print(\"Matching...\")\n",
    "matching_matrix_normal,matching_matrix_poor,match_list = match_trans(with_urls_list_normal,with_urls_list_poor,urls_dict)\n",
    "print(\"Done!\")\n",
    "print(\"Clustering isolate sessions...\")\n",
    "with_urls_list_poor = clt_final(with_urls_list_poor,wo_urls_list_poor)\n",
    "print(\"Done!\")\n",
    "print(\"Calculating results...\")\n",
    "acc_cal(with_urls_list_normal,with_urls_list_poor,match_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph matching in different networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Processing...\n",
      "Done!\n",
      "Matching...\n",
      "Done!\n",
      "Clustering isolate sessions...\n",
      "Done!\n",
      "Calculating results...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9587188612099644,\n",
       " 0.9654633852939702,\n",
       " 0.9587188612099644,\n",
       " 0.9620793030085361)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Cluster Processing...\")\n",
    "with_urls_list_normal, wo_urls_list_normal, urls_dict = clustering(\"data/InitTraffic.pkl\")\n",
    "with_urls_list_poor, wo_urls_list_poor, urls_dict = clustering(\"data/TestTraffic.pkl\")\n",
    "with_urls_list_normal, with_urls_list_poor = prepro(with_urls_list_normal,with_urls_list_poor)\n",
    "print(\"Done!\")\n",
    "print(\"Matching...\")\n",
    "matching_matrix_normal,matching_matrix_poor,match_list = match_trans(with_urls_list_normal,with_urls_list_poor,urls_dict)\n",
    "print(\"Done!\")\n",
    "print(\"Clustering isolate sessions...\")\n",
    "with_urls_list_poor = clt_final(with_urls_list_poor,wo_urls_list_poor)\n",
    "print(\"Done!\")\n",
    "print(\"Calculating results...\")\n",
    "acc_cal(with_urls_list_normal,with_urls_list_poor,match_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
